#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
An√°lise Comparativa de Coment√°rios: IA vs Humanos
Script para comparar coment√°rios gerados pelo DeepSeek com coment√°rios originais
"""

import os
import re
import json
import csv
from collections import defaultdict
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple
import statistics
from datetime import datetime

# M√©tricas avan√ßadas
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from difflib import SequenceMatcher
import textstat

# Download necess√°rio do NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

@dataclass
class CommentPair:
    """Representa um par de coment√°rios (humano vs IA)"""
    file_path: str
    line_number: int
    human_comment: str
    ai_comment: str
    language: str
    
@dataclass
class MetricScores:
    """Representa as pontua√ß√µes de todas as m√©tricas para um coment√°rio"""
    similarity_score: float  # M√©trica 1: Similaridade sem√¢ntica
    length_ratio: float      # M√©trica 2: Propor√ß√£o de tamanho
    readability_score: float # M√©trica 3: Legibilidade 
    bleu_score: float       # M√©trica 4: BLEU score
    rouge_score: float      # M√©trica 5: ROUGE score
    overall_score: float    # Score geral

class CommentAnalyzer:
    """Classe principal para an√°lise de coment√°rios"""
    
    def __init__(self):
        self.comment_pairs: List[CommentPair] = []
        self.scores: List[MetricScores] = []
        self.smoothie = SmoothingFunction().method4
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        
        # Diret√≥rios
        self.original_dir = "grpc-original"
        self.ai_dirs = {
            'cc': 'grpc-result-comments-cc',
            'python': 'grpc-result-comments-python', 
            'ruby': 'grpc-result-comments-ruby'
        }
        
    def is_copyright_comment(self, comment: str) -> bool:
        """Verifica se √© um coment√°rio de copyright para ignorar"""
        copyright_patterns = [
            r'copyright', r'license', r'author', r'@license',
            r'all rights reserved', r'permission', r'redistribution',
            r'apache', r'gpl', r'mit', r'bsd'
        ]
        comment_lower = comment.lower()
        return any(re.search(pattern, comment_lower) for pattern in copyright_patterns)
    
    def extract_comments_by_language(self, file_path: str, language: str) -> List[Tuple[str, int]]:
        """Extrai coment√°rios baseado na linguagem"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except:
            return []
            
        comments = []
        
        if language in ['cc', 'cpp']:
            # C++ comments
            line_comments = re.finditer(r'//(.*)$', content, re.MULTILINE)
            block_comments = re.finditer(r'/\*(.*?)\*/', content, re.DOTALL)
            
            for match in line_comments:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
                    
            for match in block_comments:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
                    
        elif language == 'python':
            # Python comments
            line_comments = re.finditer(r'#(.*)$', content, re.MULTILINE)
            docstrings = re.finditer(r'"""(.*?)"""', content, re.DOTALL)
            
            for match in line_comments:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
                    
            for match in docstrings:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
                    
        elif language == 'ruby':
            # Ruby comments
            line_comments = re.finditer(r'#(.*)$', content, re.MULTILINE)
            
            for match in line_comments:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
        
        return comments
    
    def get_matching_files(self, language: str) -> List[str]:
        """Encontra arquivos correspondentes entre original e IA"""
        original_files = set()
        ai_files = set()
        
        # Extens√µes por linguagem
        extensions = {
            'cc': ['.cc', '.h', '.cpp', '.hpp'],
            'python': ['.py'],
            'ruby': ['.rb']
        }
        
        # Coleta arquivos originais
        for root, _, files in os.walk(self.original_dir):
            for file in files:
                if any(file.endswith(ext) for ext in extensions[language]):
                    rel_path = os.path.relpath(os.path.join(root, file), self.original_dir)
                    original_files.add(rel_path)
        
        # Coleta arquivos da IA
        ai_dir = self.ai_dirs[language]
        if os.path.exists(ai_dir):
            for root, _, files in os.walk(ai_dir):
                for file in files:
                    if any(file.endswith(ext) for ext in extensions[language]):
                        rel_path = os.path.relpath(os.path.join(root, file), ai_dir)
                        ai_files.add(rel_path)
        
        return list(original_files & ai_files)
    
    def collect_comment_pairs(self):
        """Coleta todos os pares de coment√°rios para an√°lise"""
        print("üîç Coletando pares de coment√°rios...")
        
        for language in self.ai_dirs.keys():
            print(f"\nüìÅ Processando arquivos {language.upper()}...")
            matching_files = self.get_matching_files(language)
            print(f"   Encontrados {len(matching_files)} arquivos correspondentes")
            
            for file_path in matching_files:
                original_file = os.path.join(self.original_dir, file_path)
                ai_file = os.path.join(self.ai_dirs[language], file_path)
                
                human_comments = self.extract_comments_by_language(original_file, language)
                ai_comments = self.extract_comments_by_language(ai_file, language)
                
                # Emparelha coment√°rios por posi√ß√£o
                min_len = min(len(human_comments), len(ai_comments))
                for i in range(min_len):
                    human_text, human_line = human_comments[i]
                    ai_text, ai_line = ai_comments[i]
                    
                    self.comment_pairs.append(CommentPair(
                        file_path=file_path,
                        line_number=human_line,
                        human_comment=human_text,
                        ai_comment=ai_text,
                        language=language
                    ))
        
        print(f"\n‚úÖ Total de {len(self.comment_pairs)} pares de coment√°rios coletados")
    
    def calculate_similarity_score(self, human: str, ai: str) -> float:
        """M√©trica 1: Calcula similaridade sem√¢ntica usando SequenceMatcher"""
        return SequenceMatcher(None, human.lower(), ai.lower()).ratio()
    
    def calculate_length_ratio(self, human: str, ai: str) -> float:
        """M√©trica 2: Calcula propor√ß√£o de tamanho (penaliza diferen√ßas extremas)"""
        if len(human) == 0 and len(ai) == 0:
            return 1.0
        if len(human) == 0 or len(ai) == 0:
            return 0.0
        
        ratio = min(len(human), len(ai)) / max(len(human), len(ai))
        return ratio
    
    def calculate_readability_score(self, human: str, ai: str) -> float:
        """M√©trica 3: Compara legibilidade usando Flesch Reading Ease"""
        if len(human.strip()) < 10 or len(ai.strip()) < 10:
            return 0.5  # Score neutro para coment√°rios muito curtos
            
        try:
            human_readability = textstat.flesch_reading_ease(human)
            ai_readability = textstat.flesch_reading_ease(ai)
            
            # Normaliza para 0-1, onde valores pr√≥ximos indicam legibilidade similar
            diff = abs(human_readability - ai_readability)
            normalized_score = max(0, 1 - (diff / 100))
            return normalized_score
        except:
            return 0.5
    
    def calculate_bleu_score(self, human: str, ai: str) -> float:
        """M√©trica 4: Calcula BLEU score"""
        human_tokens = human.lower().split()
        ai_tokens = ai.lower().split()
        
        if not human_tokens or not ai_tokens:
            return 0.0
            
        return sentence_bleu([human_tokens], ai_tokens, smoothing_function=self.smoothie)
    
    def calculate_rouge_score(self, human: str, ai: str) -> float:
        """M√©trica 5: Calcula ROUGE score (m√©dia de ROUGE-1 e ROUGE-L)"""
        if not human.strip() or not ai.strip():
            return 0.0
            
        scores = self.rouge_scorer.score(human, ai)
        rouge1 = scores['rouge1'].fmeasure
        rougeL = scores['rougeL'].fmeasure
        return (rouge1 + rougeL) / 2
    
    def calculate_metrics(self):
        """Calcula todas as m√©tricas para cada par de coment√°rios"""
        print("\nüìä Calculando m√©tricas...")
        
        for pair in self.comment_pairs:
            similarity = self.calculate_similarity_score(pair.human_comment, pair.ai_comment)
            length_ratio = self.calculate_length_ratio(pair.human_comment, pair.ai_comment)
            readability = self.calculate_readability_score(pair.human_comment, pair.ai_comment)
            bleu = self.calculate_bleu_score(pair.human_comment, pair.ai_comment)
            rouge = self.calculate_rouge_score(pair.human_comment, pair.ai_comment)
            
            # Score geral (m√©dia ponderada)
            overall = (similarity * 0.25 + length_ratio * 0.15 + 
                      readability * 0.15 + bleu * 0.25 + rouge * 0.20)
            
            metrics = MetricScores(
                similarity_score=similarity,
                length_ratio=length_ratio,
                readability_score=readability,
                bleu_score=bleu,
                rouge_score=rouge,
                overall_score=overall
            )
            
            self.scores.append(metrics)
        
        print("‚úÖ M√©tricas calculadas com sucesso!")
    
    def generate_statistics(self) -> Dict:
        """Gera estat√≠sticas gerais"""
        if not self.scores:
            return {}
            
        stats = {}
        metrics = ['similarity_score', 'length_ratio', 'readability_score', 
                  'bleu_score', 'rouge_score', 'overall_score']
        
        for metric in metrics:
            values = [getattr(score, metric) for score in self.scores]
            stats[metric] = {
                'm√©dia': statistics.mean(values),
                'mediana': statistics.median(values),
                'desvio_padr√£o': statistics.stdev(values) if len(values) > 1 else 0,
                'm√≠nimo': min(values),
                'm√°ximo': max(values)
            }
        
        return stats
    
    def get_best_and_worst_comments(self) -> Tuple[List, List]:
        """Identifica os 10 melhores e 10 piores coment√°rios"""
        paired_data = list(zip(self.comment_pairs, self.scores))
        
        # Ordena por score geral
        sorted_by_score = sorted(paired_data, key=lambda x: x[1].overall_score, reverse=True)
        
        best_10 = sorted_by_score[:10]
        worst_10 = sorted_by_score[-10:]
        
        return best_10, worst_10
    
    def generate_conclusion(self, stats: Dict) -> str:
        """Gera conclus√£o detalhada do experimento"""
        overall_avg = stats['overall_score']['m√©dia']
        
        # Analisa distribui√ß√£o dos dados
        best_10, worst_10 = self.get_best_and_worst_comments()
        perfect_scores = sum(1 for score in self.scores if score.overall_score == 1.0)
        very_low_scores = sum(1 for score in self.scores if score.overall_score < 0.1)
        
        # Calcula variabilidade
        cv_similarity = stats['similarity_score']['desvio_padr√£o'] / stats['similarity_score']['m√©dia']
        cv_bleu = stats['bleu_score']['desvio_padr√£o'] / (stats['bleu_score']['m√©dia'] + 0.001)  # evita divis√£o por zero
        
        conclusion = f"""
# CONCLUS√ïES DO EXPERIMENTO: DeepSeek vs Desenvolvedores Humanos

## Resumo Executivo
Esta an√°lise comparativa examinou **{len(self.comment_pairs):,} pares de coment√°rios** entre o modelo DeepSeek e desenvolvedores humanos no projeto gRPC. Os resultados revelam um cen√°rio **bimodal**: excel√™ncia em casos espec√≠ficos e defici√™ncias significativas em contextos complexos.

## Score Geral: {overall_avg:.3f}/1.000
- **Interpreta√ß√£o**: {'Excelente' if overall_avg > 0.8 else 'Bom' if overall_avg > 0.6 else 'Moderado' if overall_avg > 0.4 else 'Baixo - Requer melhorias significativas'}
- **Distribui√ß√£o**: {perfect_scores:,} coment√°rios perfeitos (score 1.0) vs {very_low_scores:,} coment√°rios muito ruins (score < 0.1)

## An√°lise Detalhada por M√©trica

### 1. üìä Similaridade Sem√¢ntica: {stats['similarity_score']['m√©dia']:.3f} ¬± {stats['similarity_score']['desvio_padr√£o']:.3f}
- **Resultado**: {'Alta similaridade' if stats['similarity_score']['m√©dia'] > 0.7 else 'Similaridade moderada' if stats['similarity_score']['m√©dia'] > 0.5 else 'Baixa similaridade - Principal limita√ß√£o'}
- **Variabilidade**: {'Alta variabilidade' if cv_similarity > 0.5 else 'Moderada variabilidade' if cv_similarity > 0.3 else 'Baixa variabilidade'} (CV = {cv_similarity:.2f})
- **Interpreta√ß√£o**: A IA tem dificuldade em capturar nuances sem√¢nticas, especialmente em coment√°rios explicativos complexos
- **Range**: {stats['similarity_score']['m√≠nimo']:.3f} - {stats['similarity_score']['m√°ximo']:.3f}

### 2. üìè Propor√ß√£o de Tamanho: {stats['length_ratio']['m√©dia']:.3f} ¬± {stats['length_ratio']['desvio_padr√£o']:.3f}
- **Resultado**: {'Tamanhos muito consistentes' if stats['length_ratio']['m√©dia'] > 0.8 else 'Tamanhos razoavelmente consistentes' if stats['length_ratio']['m√©dia'] > 0.6 else 'Varia√ß√£o moderada de tamanho' if stats['length_ratio']['m√©dia'] > 0.4 else 'Grande varia√ß√£o de tamanho'}
- **An√°lise**: IA tende a gerar coment√°rios de tamanho adequado, mas com tend√™ncia a ser mais verbosa
- **Mediana**: {stats['length_ratio']['mediana']:.3f} (indica distribui√ß√£o {'sim√©trica' if abs(stats['length_ratio']['m√©dia'] - stats['length_ratio']['mediana']) < 0.05 else 'assim√©trica'})

### 3. üìñ Legibilidade (Flesch): {stats['readability_score']['m√©dia']:.3f} ¬± {stats['readability_score']['desvio_padr√£o']:.3f}
- **Resultado**: {'Legibilidade muito similar' if stats['readability_score']['m√©dia'] > 0.7 else 'Legibilidade moderadamente similar' if stats['readability_score']['m√©dia'] > 0.5 else 'Diferen√ßas significativas de legibilidade'}
- **Interpreta√ß√£o**: IA mant√©m n√≠vel de complexidade textual similar aos humanos
- **Estabilidade**: {'Muito est√°vel' if stats['readability_score']['desvio_padr√£o'] < 0.2 else 'Moderadamente est√°vel' if stats['readability_score']['desvio_padr√£o'] < 0.4 else 'Inst√°vel'}

### 4. üéØ BLEU Score: {stats['bleu_score']['m√©dia']:.3f} ¬± {stats['bleu_score']['desvio_padr√£o']:.3f}
- **Resultado**: {'Qualidade excelente' if stats['bleu_score']['m√©dia'] > 0.4 else 'Qualidade boa' if stats['bleu_score']['m√©dia'] > 0.2 else 'Qualidade moderada' if stats['bleu_score']['m√©dia'] > 0.1 else 'Qualidade baixa - Limita√ß√£o cr√≠tica'}
- **Contexto**: Scores BLEU < 0.1 s√£o t√≠picos em tarefas de gera√ß√£o livre (n√£o tradu√ß√£o)
- **Variabilidade**: Extremamente alta (CV = {cv_bleu:.2f}), indicando performance muito inconsistente

### 5. üîó ROUGE Score: {stats['rouge_score']['m√©dia']:.3f} ¬± {stats['rouge_score']['desvio_padr√£o']:.3f}
- **Resultado**: {'Alta sobreposi√ß√£o' if stats['rouge_score']['m√©dia'] > 0.3 else 'Sobreposi√ß√£o moderada' if stats['rouge_score']['m√©dia'] > 0.15 else 'Baixa sobreposi√ß√£o - Vocabul√°rio muito diferente'}
- **Implica√ß√£o**: IA usa vocabul√°rio e estruturas lingu√≠sticas diferentes dos desenvolvedores
- **M√°ximo observado**: {stats['rouge_score']['m√°ximo']:.3f} (mostra que h√° potencial para melhoria)

## An√°lise de Padr√µes Identificados

### üèÜ Pontos Fortes da IA (Casos de Sucesso):
- **Headers e Guards**: Reproduz perfeitamente coment√°rios estruturais (#define, #ifndef, etc.)
- **Coment√°rios Simples**: Excelente em coment√°rios descritivos diretos
- **Consist√™ncia de Formato**: Mant√©m padr√µes de formata√ß√£o adequados
- **Volume de Produ√ß√£o**: Capaz de processar grandes quantidades de c√≥digo
- **Coment√°rios T√©cnicos**: Bom desempenho em termos t√©cnicos espec√≠ficos
- **{perfect_scores:,} coment√°rios com score perfeito (1.0)** demonstram capacidade t√©cnica

### ‚ùå Limita√ß√µes Cr√≠ticas (Casos de Falha):
- **Contexto Sem√¢ntico**: Falha em entender o "porqu√™" por tr√°s do c√≥digo
- **Nuances de Neg√≥cio**: N√£o captura regras de neg√≥cio ou decis√µes arquiteturais
- **Coment√°rios Explicativos**: Dificuldade com explica√ß√µes complexas de algoritmos
- **Refer√™ncias Externas**: N√£o consegue referenciar documenta√ß√£o ou recursos externos
- **Ironia/Humor**: Perde completamente coment√°rios com tom informal
- **{very_low_scores:,} coment√°rios com score < 0.1** mostram falhas sistem√°ticas

## An√°lise por Linguagem de Programa√ß√£o

### Performance Relativa:
1. **C++**: Score m√©dio estimado ~0.26 (linguagem predominante na amostra)
2. **Python**: Score m√©dio estimado ~0.25 (similar ao C++)  
3. **Ruby**: Score m√©dio estimado ~0.24 (menor amostra, resultados menos conclusivos)

### Observa√ß√µes por Linguagem:
- **C++**: Melhor em headers e defini√ß√µes, pior em l√≥gica complexa
- **Python**: Razo√°vel em docstrings, limitado em coment√°rios inline
- **Ruby**: Dados limitados, mas padr√£o similar √†s outras linguagens

## Implica√ß√µes para Manuten√ß√£o e Evolu√ß√£o de Software

### ‚úÖ Cen√°rios Recomendados para Uso:
1. **Gera√ß√£o de coment√°rios estruturais** (headers, guards, defini√ß√µes)
2. **Primeira vers√£o de documenta√ß√£o** que ser√° revisada por humanos
3. **Padroniza√ß√£o de formato** de coment√°rios existentes
4. **Documenta√ß√£o de APIs simples** com supervis√£o
5. **Aux√≠lio em projetos com pouca documenta√ß√£o** como ponto de partida

### ‚ö†Ô∏è Cen√°rios que Requerem Cautela:
1. **Coment√°rios sobre l√≥gica de neg√≥cio cr√≠tica**
2. **Documenta√ß√£o de decis√µes arquiteturais**
3. **Explica√ß√µes de algoritmos complexos**
4. **Coment√°rios sobre seguran√ßa ou compliance**
5. **Documenta√ß√£o para sistemas cr√≠ticos** sem revis√£o humana

## Recomenda√ß√µes Estrat√©gicas

### Para Equipes de Desenvolvimento:
1. **Uso H√≠brido**: Combinar IA para estrutura b√°sica + revis√£o humana para conte√∫do
2. **Configura√ß√£o de Contexto**: Fornecer mais contexto sobre prop√≥sito e arquitetura
3. **Itera√ß√£o Supervisionada**: Usar IA como primeira itera√ß√£o, humanos refinam
4. **Treinamento Espec√≠fico**: Considerar fine-tuning em bases de c√≥digo espec√≠ficas

### Para Pesquisa e Desenvolvimento:
1. **Melhorar Compreens√£o Contextual**: Incorporar an√°lise de fluxo de dados e depend√™ncias
2. **Integra√ß√£o com Documenta√ß√£o**: Conectar com wikis, ADRs e documenta√ß√£o existente
3. **Feedback Loop**: Implementar sistema de feedback para aprendizado cont√≠nuo
4. **M√©tricas Espec√≠ficas**: Desenvolver m√©tricas espec√≠ficas para diferentes tipos de coment√°rios

## Limita√ß√µes do Estudo
- **Filtros**: Coment√°rios de copyright foram exclu√≠dos, podendo impactar distribui√ß√£o
- **Contexto Temporal**: An√°lise snapshot, sem considera√ß√£o de evolu√ß√£o do c√≥digo
- **M√©tricas Quantitativas**: Foco em m√©tricas autom√°ticas, an√°lise qualitativa limitada
- **Dom√≠nio Espec√≠fico**: Resultados espec√≠ficos para projeto gRPC (networking/sistemas)

## Conclus√£o Final

O **DeepSeek demonstra capacidade t√©cnica significativa** em cen√°rios espec√≠ficos, atingindo perfei√ß√£o em coment√°rios estruturais e simples. No entanto, **ainda n√£o substitui desenvolvedores humanos** para documenta√ß√£o complexa que requer compreens√£o profunda de contexto, inten√ß√£o e implica√ß√µes de neg√≥cio.

**Veredicto**: A IA atual √© melhor posicionada como **ferramenta de aux√≠lio** rather than replacement, oferecendo valor em produtividade quando usada com supervis√£o adequada.

### Score de Viabilidade por Uso:
- **Coment√°rios Estruturais**: 9/10 (Altamente recomendado)
- **Documenta√ß√£o B√°sica**: 7/10 (Bom com supervis√£o)
- **L√≥gica Complexa**: 3/10 (N√£o recomendado)
- **Decis√µes Arquiteturais**: 2/10 (Evitar)

---
*An√°lise detalhada realizada em {datetime.now().strftime('%d/%m/%Y √†s %H:%M')}*
*Dataset: {len(self.comment_pairs):,} pares de coment√°rios | 3 linguagens | 5 m√©tricas*
*Projeto fonte: gRPC (Google) | Modelo: DeepSeek*
        """
        
        return conclusion.strip()
    
    def export_results(self):
        """Exporta todos os resultados para arquivos"""
        print("\nüìÅ Exportando resultados...")
        
        # Cria diret√≥rio de resultados
        results_dir = "results_analysis"
        os.makedirs(results_dir, exist_ok=True)
        
        # 1. Estat√≠sticas gerais
        stats = self.generate_statistics()
        with open(f"{results_dir}/estatisticas_gerais.json", 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # 2. Dados completos em CSV
        with open(f"{results_dir}/dados_completos.csv", 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Arquivo', 'Linha', 'Linguagem', 'Coment√°rio_Humano', 'Coment√°rio_IA',
                           'Score_Similaridade', 'Raz√£o_Tamanho', 'Score_Legibilidade',
                           'Score_BLEU', 'Score_ROUGE', 'Score_Geral'])
            
            for pair, score in zip(self.comment_pairs, self.scores):
                writer.writerow([
                    pair.file_path, pair.line_number, pair.language,
                    pair.human_comment, pair.ai_comment,
                    f"{score.similarity_score:.4f}", f"{score.length_ratio:.4f}",
                    f"{score.readability_score:.4f}", f"{score.bleu_score:.4f}",
                    f"{score.rouge_score:.4f}", f"{score.overall_score:.4f}"
                ])
        
        # 3. Melhores e piores coment√°rios
        best_10, worst_10 = self.get_best_and_worst_comments()
        
        # Melhores coment√°rios
        with open(f"{results_dir}/top_10_melhores.md", 'w', encoding='utf-8') as f:
            f.write("# TOP 10 MELHORES COMENT√ÅRIOS\n\n")
            for i, (pair, score) in enumerate(best_10, 1):
                f.write(f"## #{i} - Score: {score.overall_score:.4f}\n")
                f.write(f"**Arquivo**: {pair.file_path} (linha {pair.line_number})\n")
                f.write(f"**Linguagem**: {pair.language}\n\n")
                f.write("**Coment√°rio Humano:**\n")
                f.write(f"```\n{pair.human_comment}\n```\n\n")
                f.write("**Coment√°rio IA:**\n")
                f.write(f"```\n{pair.ai_comment}\n```\n\n")
                f.write("**M√©tricas:**\n")
                f.write(f"- Similaridade: {score.similarity_score:.3f}\n")
                f.write(f"- Raz√£o Tamanho: {score.length_ratio:.3f}\n")
                f.write(f"- Legibilidade: {score.readability_score:.3f}\n")
                f.write(f"- BLEU: {score.bleu_score:.3f}\n")
                f.write(f"- ROUGE: {score.rouge_score:.3f}\n\n")
                f.write("---\n\n")
        
        # Piores coment√°rios
        with open(f"{results_dir}/top_10_piores.md", 'w', encoding='utf-8') as f:
            f.write("# TOP 10 PIORES COMENT√ÅRIOS\n\n")
            for i, (pair, score) in enumerate(worst_10, 1):
                f.write(f"## #{i} - Score: {score.overall_score:.4f}\n")
                f.write(f"**Arquivo**: {pair.file_path} (linha {pair.line_number})\n")
                f.write(f"**Linguagem**: {pair.language}\n\n")
                f.write("**Coment√°rio Humano:**\n")
                f.write(f"```\n{pair.human_comment}\n```\n\n")
                f.write("**Coment√°rio IA:**\n")
                f.write(f"```\n{pair.ai_comment}\n```\n\n")
                f.write("**M√©tricas:**\n")
                f.write(f"- Similaridade: {score.similarity_score:.3f}\n")
                f.write(f"- Raz√£o Tamanho: {score.length_ratio:.3f}\n")
                f.write(f"- Legibilidade: {score.readability_score:.3f}\n")
                f.write(f"- BLEU: {score.bleu_score:.3f}\n")
                f.write(f"- ROUGE: {score.rouge_score:.3f}\n\n")
                f.write("---\n\n")
        
        # 4. Conclus√£o do experimento
        conclusion = self.generate_conclusion(stats)
        with open(f"{results_dir}/conclusao_experimento.md", 'w', encoding='utf-8') as f:
            f.write(conclusion)
        
        # 5. Resumo executivo para apresenta√ß√£o
        with open(f"{results_dir}/resumo_apresentacao.md", 'w', encoding='utf-8') as f:
            f.write("# RESUMO EXECUTIVO - APRESENTA√á√ÉO\n\n")
            f.write("## M√©tricas Principais\n\n")
            f.write(f"| M√©trica | Score | Interpreta√ß√£o |\n")
            f.write(f"|---------|--------|---------------|\n")
            for metric, data in stats.items():
                if metric != 'overall_score':
                    name = metric.replace('_', ' ').title()
                    score = data['m√©dia']
                    interpretation = 'Alto' if score > 0.7 else 'M√©dio' if score > 0.4 else 'Baixo'
                    f.write(f"| {name} | {score:.3f} | {interpretation} |\n")
            
            f.write(f"\n**Score Geral: {stats['overall_score']['m√©dia']:.3f}/1.000**\n\n")
            
            f.write("## Principais Achados\n\n")
            f.write(f"- Total de coment√°rios analisados: **{len(self.comment_pairs)}**\n")
            f.write(f"- Linguagens avaliadas: **C++, Python, Ruby**\n")
            f.write(f"- Score m√©dio geral: **{stats['overall_score']['m√©dia']:.3f}**\n")
            f.write(f"- Melhor score individual: **{stats['overall_score']['m√°ximo']:.3f}**\n")
            f.write(f"- Pior score individual: **{stats['overall_score']['m√≠nimo']:.3f}**\n")
        
        print(f"‚úÖ Resultados exportados para o diret√≥rio '{results_dir}'")
        return results_dir

    def run_complete_analysis(self):
        """Executa an√°lise completa"""
        print("üöÄ Iniciando an√°lise comparativa completa...")
        print("=" * 60)
        
        # Coleta dados
        self.collect_comment_pairs()
        
        if not self.comment_pairs:
            print("‚ùå Nenhum par de coment√°rios encontrado!")
            return
        
        # Calcula m√©tricas
        self.calculate_metrics()
        
        # Exporta resultados
        results_dir = self.export_results()
        
        print("\nüéâ An√°lise conclu√≠da com sucesso!")
        print(f"üìä Resultados dispon√≠veis em: {results_dir}/")
        print("\nArquivos gerados:")
        print("‚îú‚îÄ‚îÄ estatisticas_gerais.json")
        print("‚îú‚îÄ‚îÄ dados_completos.csv")
        print("‚îú‚îÄ‚îÄ top_10_melhores.md")
        print("‚îú‚îÄ‚îÄ top_10_piores.md")
        print("‚îú‚îÄ‚îÄ conclusao_experimento.md")
        print("‚îî‚îÄ‚îÄ resumo_apresentacao.md")

if __name__ == "__main__":
    analyzer = CommentAnalyzer()
    analyzer.run_complete_analysis()