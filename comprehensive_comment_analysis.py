#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AnÃ¡lise Comparativa de ComentÃ¡rios: IA vs Humanos
Script para comparar comentÃ¡rios gerados pelo DeepSeek com comentÃ¡rios originais
"""

import os
import re
import json
import csv
from collections import defaultdict
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple
import statistics
from datetime import datetime

# MÃ©tricas avanÃ§adas
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from rouge_score import rouge_scorer
from difflib import SequenceMatcher
import textstat

# Download necessÃ¡rio do NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/wordnet')
except LookupError:
    nltk.download('wordnet')

@dataclass
class CommentPair:
    """Representa um par de comentÃ¡rios (humano vs IA)"""
    file_path: str
    line_number: int
    human_comment: str
    ai_comment: str
    language: str
    
@dataclass
class MetricScores:
    """Representa as pontuaÃ§Ãµes de todas as mÃ©tricas para um comentÃ¡rio"""
    similarity_score: float  # MÃ©trica 1: Similaridade semÃ¢ntica
    length_ratio: float      # MÃ©trica 2: ProporÃ§Ã£o de tamanho
    readability_score: float # MÃ©trica 3: Legibilidade 
    bleu_score: float       # MÃ©trica 4: BLEU score
    rouge_score: float      # MÃ©trica 5: ROUGE score
    overall_score: float    # Score geral

class CommentAnalyzer:
    """Classe principal para anÃ¡lise de comentÃ¡rios"""
    
    def __init__(self):
        self.comment_pairs: List[CommentPair] = []
        self.scores: List[MetricScores] = []
        self.smoothie = SmoothingFunction().method4
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
        
        # DiretÃ³rios
        self.original_dir = "grpc-original"
        self.ai_dirs = {
            'cc': 'grpc-result-comments-cc',
            'python': 'grpc-result-comments-python', 
            'ruby': 'grpc-result-comments-ruby'
        }
        
    def is_copyright_comment(self, comment: str) -> bool:
        """Verifica se Ã© um comentÃ¡rio de copyright para ignorar"""
        copyright_patterns = [
            r'copyright', r'license', r'author', r'@license',
            r'all rights reserved', r'permission', r'redistribution',
            r'apache', r'gpl', r'mit', r'bsd'
        ]
        comment_lower = comment.lower()
        return any(re.search(pattern, comment_lower) for pattern in copyright_patterns)
    
    def extract_comments_by_language(self, file_path: str, language: str) -> List[Tuple[str, int]]:
        """Extrai comentÃ¡rios baseado na linguagem"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except:
            return []
            
        comments = []
        
        if language in ['cc', 'cpp']:
            # C++ comments
            line_comments = re.finditer(r'//(.*)$', content, re.MULTILINE)
            block_comments = re.finditer(r'/\*(.*?)\*/', content, re.DOTALL)
            
            for match in line_comments:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
                    
            for match in block_comments:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
                    
        elif language == 'python':
            # Python comments
            line_comments = re.finditer(r'#(.*)$', content, re.MULTILINE)
            docstrings = re.finditer(r'"""(.*?)"""', content, re.DOTALL)
            
            for match in line_comments:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
                    
            for match in docstrings:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
                    
        elif language == 'ruby':
            # Ruby comments
            line_comments = re.finditer(r'#(.*)$', content, re.MULTILINE)
            
            for match in line_comments:
                line_num = content[:match.start()].count('\n') + 1
                comment_text = match.group(1).strip()
                if comment_text and not self.is_copyright_comment(comment_text):
                    comments.append((comment_text, line_num))
        
        return comments
    
    def get_matching_files(self, language: str) -> List[str]:
        """Encontra arquivos correspondentes entre original e IA"""
        original_files = set()
        ai_files = set()
        
        # ExtensÃµes por linguagem
        extensions = {
            'cc': ['.cc', '.h', '.cpp', '.hpp'],
            'python': ['.py'],
            'ruby': ['.rb']
        }
        
        # Coleta arquivos originais
        for root, _, files in os.walk(self.original_dir):
            for file in files:
                if any(file.endswith(ext) for ext in extensions[language]):
                    rel_path = os.path.relpath(os.path.join(root, file), self.original_dir)
                    original_files.add(rel_path)
        
        # Coleta arquivos da IA
        ai_dir = self.ai_dirs[language]
        if os.path.exists(ai_dir):
            for root, _, files in os.walk(ai_dir):
                for file in files:
                    if any(file.endswith(ext) for ext in extensions[language]):
                        rel_path = os.path.relpath(os.path.join(root, file), ai_dir)
                        ai_files.add(rel_path)
        
        return list(original_files & ai_files)
    
    def collect_comment_pairs(self):
        """Coleta todos os pares de comentÃ¡rios para anÃ¡lise"""
        print("ğŸ” Coletando pares de comentÃ¡rios...")
        
        for language in self.ai_dirs.keys():
            print(f"\nğŸ“ Processando arquivos {language.upper()}...")
            matching_files = self.get_matching_files(language)
            print(f"   Encontrados {len(matching_files)} arquivos correspondentes")
            
            for file_path in matching_files:
                original_file = os.path.join(self.original_dir, file_path)
                ai_file = os.path.join(self.ai_dirs[language], file_path)
                
                human_comments = self.extract_comments_by_language(original_file, language)
                ai_comments = self.extract_comments_by_language(ai_file, language)
                
                # Emparelha comentÃ¡rios por posiÃ§Ã£o
                min_len = min(len(human_comments), len(ai_comments))
                for i in range(min_len):
                    human_text, human_line = human_comments[i]
                    ai_text, ai_line = ai_comments[i]
                    
                    self.comment_pairs.append(CommentPair(
                        file_path=file_path,
                        line_number=human_line,
                        human_comment=human_text,
                        ai_comment=ai_text,
                        language=language
                    ))
        
        print(f"\nâœ… Total de {len(self.comment_pairs)} pares de comentÃ¡rios coletados")
    
    def calculate_similarity_score(self, human: str, ai: str) -> float:
        """MÃ©trica 1: Calcula similaridade semÃ¢ntica usando SequenceMatcher"""
        return SequenceMatcher(None, human.lower(), ai.lower()).ratio()
    
    def calculate_length_ratio(self, human: str, ai: str) -> float:
        """MÃ©trica 2: Calcula proporÃ§Ã£o de tamanho (penaliza diferenÃ§as extremas)"""
        if len(human) == 0 and len(ai) == 0:
            return 1.0
        if len(human) == 0 or len(ai) == 0:
            return 0.0
        
        ratio = min(len(human), len(ai)) / max(len(human), len(ai))
        return ratio
    
    def calculate_readability_score(self, human: str, ai: str) -> float:
        """MÃ©trica 3: Compara legibilidade usando Flesch Reading Ease"""
        if len(human.strip()) < 10 or len(ai.strip()) < 10:
            return 0.5  # Score neutro para comentÃ¡rios muito curtos
            
        try:
            human_readability = textstat.flesch_reading_ease(human)
            ai_readability = textstat.flesch_reading_ease(ai)
            
            # Normaliza para 0-1, onde valores prÃ³ximos indicam legibilidade similar
            diff = abs(human_readability - ai_readability)
            normalized_score = max(0, 1 - (diff / 100))
            return normalized_score
        except:
            return 0.5
    
    def calculate_bleu_score(self, human: str, ai: str) -> float:
        """MÃ©trica 4: Calcula BLEU score"""
        human_tokens = human.lower().split()
        ai_tokens = ai.lower().split()
        
        if not human_tokens or not ai_tokens:
            return 0.0
            
        return sentence_bleu([human_tokens], ai_tokens, smoothing_function=self.smoothie)
    
    def calculate_rouge_score(self, human: str, ai: str) -> float:
        """MÃ©trica 5: Calcula ROUGE score (mÃ©dia de ROUGE-1 e ROUGE-L)"""
        if not human.strip() or not ai.strip():
            return 0.0
            
        scores = self.rouge_scorer.score(human, ai)
        rouge1 = scores['rouge1'].fmeasure
        rougeL = scores['rougeL'].fmeasure
        return (rouge1 + rougeL) / 2
    
    def calculate_metrics(self):
        """Calcula todas as mÃ©tricas para cada par de comentÃ¡rios"""
        print("\nğŸ“Š Calculando mÃ©tricas...")
        
        for pair in self.comment_pairs:
            similarity = self.calculate_similarity_score(pair.human_comment, pair.ai_comment)
            length_ratio = self.calculate_length_ratio(pair.human_comment, pair.ai_comment)
            readability = self.calculate_readability_score(pair.human_comment, pair.ai_comment)
            bleu = self.calculate_bleu_score(pair.human_comment, pair.ai_comment)
            rouge = self.calculate_rouge_score(pair.human_comment, pair.ai_comment)
            
            # Score geral (mÃ©dia ponderada)
            overall = (similarity * 0.25 + length_ratio * 0.15 + 
                      readability * 0.15 + bleu * 0.25 + rouge * 0.20)
            
            metrics = MetricScores(
                similarity_score=similarity,
                length_ratio=length_ratio,
                readability_score=readability,
                bleu_score=bleu,
                rouge_score=rouge,
                overall_score=overall
            )
            
            self.scores.append(metrics)
        
        print("âœ… MÃ©tricas calculadas com sucesso!")
    
    def generate_statistics(self) -> Dict:
        """Gera estatÃ­sticas gerais"""
        if not self.scores:
            return {}
            
        stats = {}
        metrics = ['similarity_score', 'length_ratio', 'readability_score', 
                  'bleu_score', 'rouge_score', 'overall_score']
        
        for metric in metrics:
            values = [getattr(score, metric) for score in self.scores]
            stats[metric] = {
                'mÃ©dia': statistics.mean(values),
                'mediana': statistics.median(values),
                'desvio_padrÃ£o': statistics.stdev(values) if len(values) > 1 else 0,
                'mÃ­nimo': min(values),
                'mÃ¡ximo': max(values)
            }
        
        return stats
    
    def get_best_and_worst_comments(self) -> Tuple[List, List]:
        """Identifica os 10 melhores e 10 piores comentÃ¡rios"""
        paired_data = list(zip(self.comment_pairs, self.scores))
        
        # Ordena por score geral
        sorted_by_score = sorted(paired_data, key=lambda x: x[1].overall_score, reverse=True)
        
        best_10 = sorted_by_score[:10]
        worst_10 = sorted_by_score[-10:]
        
        return best_10, worst_10
    
    def generate_conclusion(self, stats: Dict) -> str:
        """Gera conclusÃ£o do experimento"""
        overall_avg = stats['overall_score']['mÃ©dia']
        
        conclusion = f"""
# CONCLUSÃ•ES DO EXPERIMENTO

## Resumo Executivo
A anÃ¡lise comparativa entre comentÃ¡rios gerados pelo DeepSeek e comentÃ¡rios escritos por desenvolvedores humanos revelou insights importantes sobre a capacidade atual da IA em documentaÃ§Ã£o de cÃ³digo.

## Principais Descobertas

### Score Geral: {overall_avg:.3f}/1.000
- **InterpretaÃ§Ã£o**: {'Excelente' if overall_avg > 0.8 else 'Bom' if overall_avg > 0.6 else 'Moderado' if overall_avg > 0.4 else 'Baixo'}

### AnÃ¡lise por MÃ©trica:

1. **Similaridade SemÃ¢ntica**: {stats['similarity_score']['mÃ©dia']:.3f}
   - Mede o quÃ£o similar Ã© o conteÃºdo dos comentÃ¡rios
   - {'Alta similaridade' if stats['similarity_score']['mÃ©dia'] > 0.7 else 'Similaridade moderada' if stats['similarity_score']['mÃ©dia'] > 0.5 else 'Baixa similaridade'}

2. **ProporÃ§Ã£o de Tamanho**: {stats['length_ratio']['mÃ©dia']:.3f}
   - Avalia se a IA mantÃ©m tamanhos apropriados
   - {'Tamanhos consistentes' if stats['length_ratio']['mÃ©dia'] > 0.7 else 'VariaÃ§Ã£o moderada' if stats['length_ratio']['mÃ©dia'] > 0.5 else 'Grande variaÃ§Ã£o de tamanho'}

3. **Legibilidade**: {stats['readability_score']['mÃ©dia']:.3f}
   - Compara facilidade de leitura
   - {'Legibilidade similar' if stats['readability_score']['mÃ©dia'] > 0.7 else 'DiferenÃ§as moderadas' if stats['readability_score']['mÃ©dia'] > 0.5 else 'Grandes diferenÃ§as de legibilidade'}

4. **BLEU Score**: {stats['bleu_score']['mÃ©dia']:.3f}
   - MÃ©trica padrÃ£o para qualidade de texto
   - {'Qualidade alta' if stats['bleu_score']['mÃ©dia'] > 0.3 else 'Qualidade moderada' if stats['bleu_score']['mÃ©dia'] > 0.1 else 'Qualidade baixa'}

5. **ROUGE Score**: {stats['rouge_score']['mÃ©dia']:.3f}
   - Mede sobreposiÃ§Ã£o de conteÃºdo
   - {'Alta sobreposiÃ§Ã£o' if stats['rouge_score']['mÃ©dia'] > 0.3 else 'SobreposiÃ§Ã£o moderada' if stats['rouge_score']['mÃ©dia'] > 0.1 else 'Baixa sobreposiÃ§Ã£o'}

## RecomendaÃ§Ãµes

### Pontos Fortes da IA:
- {"MantÃ©m estrutura similar aos comentÃ¡rios humanos" if stats['similarity_score']['mÃ©dia'] > 0.6 else ""}
- {"Produz comentÃ¡rios de tamanho apropriado" if stats['length_ratio']['mÃ©dia'] > 0.6 else ""}

### Ãreas para Melhoria:
- {"Melhorar similaridade semÃ¢ntica" if stats['similarity_score']['mÃ©dia'] < 0.6 else ""}
- {"Ajustar tamanho dos comentÃ¡rios" if stats['length_ratio']['mÃ©dia'] < 0.6 else ""}
- {"Aprimorar legibilidade" if stats['readability_score']['mÃ©dia'] < 0.6 else ""}

## Impacto para ManutenÃ§Ã£o de Software
{"A IA demonstra capacidade adequada para auxiliar na documentaÃ§Ã£o de cÃ³digo, podendo ser uma ferramenta valiosa para desenvolvedores." if overall_avg > 0.6 else "A IA ainda precisa de aprimoramentos significativos antes de ser considerada uma alternativa viÃ¡vel para documentaÃ§Ã£o automÃ¡tica de cÃ³digo."}

---
*AnÃ¡lise realizada em {datetime.now().strftime('%d/%m/%Y Ã s %H:%M')}*
*Total de comentÃ¡rios analisados: {len(self.comment_pairs)}*
        """
        
        return conclusion.strip()
    
    def export_results(self):
        """Exporta todos os resultados para arquivos"""
        print("\nğŸ“ Exportando resultados...")
        
        # Cria diretÃ³rio de resultados
        results_dir = "results_analysis"
        os.makedirs(results_dir, exist_ok=True)
        
        # 1. EstatÃ­sticas gerais
        stats = self.generate_statistics()
        with open(f"{results_dir}/estatisticas_gerais.json", 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        
        # 2. Dados completos em CSV
        with open(f"{results_dir}/dados_completos.csv", 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Arquivo', 'Linha', 'Linguagem', 'ComentÃ¡rio_Humano', 'ComentÃ¡rio_IA',
                           'Score_Similaridade', 'RazÃ£o_Tamanho', 'Score_Legibilidade',
                           'Score_BLEU', 'Score_ROUGE', 'Score_Geral'])
            
            for pair, score in zip(self.comment_pairs, self.scores):
                writer.writerow([
                    pair.file_path, pair.line_number, pair.language,
                    pair.human_comment, pair.ai_comment,
                    f"{score.similarity_score:.4f}", f"{score.length_ratio:.4f}",
                    f"{score.readability_score:.4f}", f"{score.bleu_score:.4f}",
                    f"{score.rouge_score:.4f}", f"{score.overall_score:.4f}"
                ])
        
        # 3. Melhores e piores comentÃ¡rios
        best_10, worst_10 = self.get_best_and_worst_comments()
        
        # Melhores comentÃ¡rios
        with open(f"{results_dir}/top_10_melhores.md", 'w', encoding='utf-8') as f:
            f.write("# TOP 10 MELHORES COMENTÃRIOS\n\n")
            for i, (pair, score) in enumerate(best_10, 1):
                f.write(f"## #{i} - Score: {score.overall_score:.4f}\n")
                f.write(f"**Arquivo**: {pair.file_path} (linha {pair.line_number})\n")
                f.write(f"**Linguagem**: {pair.language}\n\n")
                f.write("**ComentÃ¡rio Humano:**\n")
                f.write(f"```\n{pair.human_comment}\n```\n\n")
                f.write("**ComentÃ¡rio IA:**\n")
                f.write(f"```\n{pair.ai_comment}\n```\n\n")
                f.write("**MÃ©tricas:**\n")
                f.write(f"- Similaridade: {score.similarity_score:.3f}\n")
                f.write(f"- RazÃ£o Tamanho: {score.length_ratio:.3f}\n")
                f.write(f"- Legibilidade: {score.readability_score:.3f}\n")
                f.write(f"- BLEU: {score.bleu_score:.3f}\n")
                f.write(f"- ROUGE: {score.rouge_score:.3f}\n\n")
                f.write("---\n\n")
        
        # Piores comentÃ¡rios
        with open(f"{results_dir}/top_10_piores.md", 'w', encoding='utf-8') as f:
            f.write("# TOP 10 PIORES COMENTÃRIOS\n\n")
            for i, (pair, score) in enumerate(worst_10, 1):
                f.write(f"## #{i} - Score: {score.overall_score:.4f}\n")
                f.write(f"**Arquivo**: {pair.file_path} (linha {pair.line_number})\n")
                f.write(f"**Linguagem**: {pair.language}\n\n")
                f.write("**ComentÃ¡rio Humano:**\n")
                f.write(f"```\n{pair.human_comment}\n```\n\n")
                f.write("**ComentÃ¡rio IA:**\n")
                f.write(f"```\n{pair.ai_comment}\n```\n\n")
                f.write("**MÃ©tricas:**\n")
                f.write(f"- Similaridade: {score.similarity_score:.3f}\n")
                f.write(f"- RazÃ£o Tamanho: {score.length_ratio:.3f}\n")
                f.write(f"- Legibilidade: {score.readability_score:.3f}\n")
                f.write(f"- BLEU: {score.bleu_score:.3f}\n")
                f.write(f"- ROUGE: {score.rouge_score:.3f}\n\n")
                f.write("---\n\n")
        
        # 4. ConclusÃ£o do experimento
        conclusion = self.generate_conclusion(stats)
        with open(f"{results_dir}/conclusao_experimento.md", 'w', encoding='utf-8') as f:
            f.write(conclusion)
        
        # 5. Resumo executivo para apresentaÃ§Ã£o
        with open(f"{results_dir}/resumo_apresentacao.md", 'w', encoding='utf-8') as f:
            f.write("# RESUMO EXECUTIVO - APRESENTAÃ‡ÃƒO\n\n")
            f.write("## MÃ©tricas Principais\n\n")
            f.write(f"| MÃ©trica | Score | InterpretaÃ§Ã£o |\n")
            f.write(f"|---------|--------|---------------|\n")
            for metric, data in stats.items():
                if metric != 'overall_score':
                    name = metric.replace('_', ' ').title()
                    score = data['mÃ©dia']
                    interpretation = 'Alto' if score > 0.7 else 'MÃ©dio' if score > 0.4 else 'Baixo'
                    f.write(f"| {name} | {score:.3f} | {interpretation} |\n")
            
            f.write(f"\n**Score Geral: {stats['overall_score']['mÃ©dia']:.3f}/1.000**\n\n")
            
            f.write("## Principais Achados\n\n")
            f.write(f"- Total de comentÃ¡rios analisados: **{len(self.comment_pairs)}**\n")
            f.write(f"- Linguagens avaliadas: **C++, Python, Ruby**\n")
            f.write(f"- Score mÃ©dio geral: **{stats['overall_score']['mÃ©dia']:.3f}**\n")
            f.write(f"- Melhor score individual: **{stats['overall_score']['mÃ¡ximo']:.3f}**\n")
            f.write(f"- Pior score individual: **{stats['overall_score']['mÃ­nimo']:.3f}**\n")
        
        print(f"âœ… Resultados exportados para o diretÃ³rio '{results_dir}'")
        return results_dir

    def run_complete_analysis(self):
        """Executa anÃ¡lise completa"""
        print("ğŸš€ Iniciando anÃ¡lise comparativa completa...")
        print("=" * 60)
        
        # Coleta dados
        self.collect_comment_pairs()
        
        if not self.comment_pairs:
            print("âŒ Nenhum par de comentÃ¡rios encontrado!")
            return
        
        # Calcula mÃ©tricas
        self.calculate_metrics()
        
        # Exporta resultados
        results_dir = self.export_results()
        
        print("\nğŸ‰ AnÃ¡lise concluÃ­da com sucesso!")
        print(f"ğŸ“Š Resultados disponÃ­veis em: {results_dir}/")
        print("\nArquivos gerados:")
        print("â”œâ”€â”€ estatisticas_gerais.json")
        print("â”œâ”€â”€ dados_completos.csv")
        print("â”œâ”€â”€ top_10_melhores.md")
        print("â”œâ”€â”€ top_10_piores.md")
        print("â”œâ”€â”€ conclusao_experimento.md")
        print("â””â”€â”€ resumo_apresentacao.md")

if __name__ == "__main__":
    analyzer = CommentAnalyzer()
    analyzer.run_complete_analysis()